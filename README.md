
# CredTech ‚Äî Real‚ÄëTime Explainable Credit Intelligence

A polished, hackathon‚Äëready platform that ingests multi‚Äësource data, produces dynamic **issuer‚Äëlevel creditworthiness scores**, and explains every score with **feature‚Äëlevel attributions** (SHAP) and **news‚Äëdriven context**. Ships with a **Streamlit dashboard**, robust pipeline, and Docker support.

## ‚ú® Highlights
- **Multi‚Äësource ingestion** (equities, sector ETF, macro index, commodity; plus **news headlines**).
- **Adaptive scoring engine** (LightGBM) retrained on every refresh.
- **Explainability layer** with **SHAP** (waterfall + beeswarm) and **plain‚Äëlanguage** drivers.
- **Unstructured data integration** via VADER (default) or **FinBERT** (optional).
- **Analyst dashboard**: trend lines, alerts for sudden changes, news feed, and agency‚Äërating overlay.
- **Error‚Äëtolerant** pipeline with graceful fallbacks and clear logging.

> This codebase aligns with the hackathon problem statement: robust data engineering, explainable scoring, unstructured event integration, a usable dashboard, and deployment readiness.

---

## üß± Project Structure
```
credtech_hackathon/
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                # downloaded CSVs (prices, news)
‚îÇ  ‚îî‚îÄ processed/          # feature matrices, news with sentiment
‚îú‚îÄ models/                # trained models + SHAP bundles
‚îú‚îÄ data_ingestion/
‚îÇ  ‚îú‚îÄ yfinance_ingestor.py
‚îÇ  ‚îî‚îÄ news_ingestor.py
‚îú‚îÄ feature_engineering/
‚îÇ  ‚îú‚îÄ structured_features.py
‚îÇ  ‚îî‚îÄ unstructured_features.py
‚îú‚îÄ modeling/
‚îÇ  ‚îú‚îÄ train.py
‚îÇ  ‚îî‚îÄ explain.py
‚îú‚îÄ utils/
‚îÇ  ‚îú‚îÄ logging_utils.py
‚îÇ  ‚îî‚îÄ mock_data_generator.py
‚îú‚îÄ config.py
‚îú‚îÄ main.py                # orchestrates the whole pipeline
‚îú‚îÄ dashboard.py           # Streamlit app
‚îî‚îÄ requirements.txt
```

---

## üöÄ Quickstart (Local)

### 1) Create and activate a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate    # on Windows: .venv\Scripts\activate
```

### 2) Install dependencies
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

> The default configuration uses **VADER** for sentiment (lightweight). To use **FinBERT**, set `USE_LIGHT_NLP = False` in `config.py` and ensure `transformers` and `torch` are installed (already in `requirements.txt`).

### 3) (Optional) Provide a News API Key
If you have a [newsapi.org](https://newsapi.org) key, export it so news ingestion uses it:
```bash
export NEWS_API_KEY="YOUR_KEY_HERE"   # Windows PowerShell: $env:NEWS_API_KEY="YOUR_KEY_HERE"
```

> No key? No problem. The pipeline will fall back to `yfinance`'s built‚Äëin news, and if that is unavailable, a bundled `sample_news.csv` is used so the app always runs.

### 4) Run the end‚Äëto‚Äëend pipeline
```bash
python main.py
```
This will:
- download market data (equities, sector ETF, macro index, crude futures),
- ingest news,
- compute sentiment and features,
- train a LightGBM model per issuer,
- pre‚Äëcompute SHAP values, and
- generate a mocked agency‚Äërating series for overlay.

### 5) Launch the dashboard
```bash
streamlit run dashboard.py
```
Open the URL shown (usually http://localhost:8501).

---

## üß† How the Score Works (Demo Mode)
We compute a **synthetic, but realistic** credit score combining: lower 30D volatility (good), positive price momentum (good), supportive sector & macro trends (good), and rising crude (generally bad for costs). **News sentiment** provides an incremental adjustment. We **train LightGBM** to fit this target so **SHAP** explanations are meaningful and fast.

> In a production system, you would replace the synthetic target with **historical credit events/ratings** or proprietary labels.

---

## üß© Dashboard Features
- **Overview** tab:
  - Dynamic score trend with **agency rating overlay** (step line).
  - KPI cards: latest score, 30D volatility, recent news sentiment.
  - **News feed** per issuer.
- **Score Deconstruction** tab:
  - **SHAP waterfall** for a selected date.
  - **Plain‚Äëlanguage** bullet points extracted from the top SHAP contributors.
  - **SHAP beeswarm** for global importance.
- **Data Explorer** tab: visualize any input feature.
- **Alerts** (sidebar): flags issuers with **sudden score changes** (configurable threshold).

---

## ‚öôÔ∏è Configuration
Edit `config.py`:
- Universe of tickers (`ISSUERS`, `SECTOR_ETFS`, `MACRO_TICKERS`, `COMMODITY_TICKERS`).
- Dates, model parameters, and NLP mode (`USE_LIGHT_NLP`).

---

## üß™ Troubleshooting
- **No charts / artifacts missing** ‚Üí run `python main.py` first.
- **SHAP plots not showing** ‚Üí ensure **shap** and **streamlit‚Äëshap** installed, then refresh.
- **FinBERT too slow / heavy** ‚Üí leave `USE_LIGHT_NLP = True` (default VADER).

---

## üê≥ Docker (Optional)
```dockerfile
# Dockerfile (example)
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt && python - <<'PY'\nimport nltk;nltk.download('vader_lexicon')\nPY
COPY . .
EXPOSE 8501
ENV PYTHONUNBUFFERED=1
CMD ["bash","-lc","python main.py && streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0"]
```
Build & run:
```bash
docker build -t credtech .
docker run -p 8501:8501 -e NEWS_API_KEY=$NEWS_API_KEY credtech
```

---

## üßØ Design Notes & Trade‚Äëoffs
- **Speed vs Accuracy**: LightGBM chosen for fast retrains with strong tabular performance.
- **Data sources**: to keep the project **key‚Äëfree** by default, market & commodity prices use `yfinance` tickers; news uses a **graceful fallback** chain.
- **Explainability**: **model‚Äëintrinsic** SHAP on tree ensembles, not LLM‚Äëbased summaries.
- **Storage**: file‚Äëbased CSVs for hackathon simplicity; swap for DB in production.
- **Scheduling**: run `main.py` via cron/GitHub Actions for daily refresh; adopt Airflow/Prefect later.

---

## üìú License
MIT (or adapt per your needs for the hackathon).
